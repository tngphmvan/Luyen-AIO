{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee4368d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d81b48ff7d47939de3f8969ec51bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0817eda86841a294201ffb64c561ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-zh/test-00000-of-00001.parquet:   0%|          | 0.00/355k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c5f775f6884f2b944dfdfc7534b4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-zh/train-00000-of-00001.parquet:   0%|          | 0.00/143M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018d52059ad9452ba43ad479c475e223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-zh/validation-00000-of-00001.parquet:   0%|          | 0.00/359k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cd470f4a824f82af92b88bb957f736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddf1d558e414e2fa28b3d08a803fe95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6147c3dd2849d49a744c03324b99c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/opus-100\", \"en-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1034cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([{'en': 'Sixty-first session', 'zh': '第六十一届会议'}, {'en': 'I took some medicine for my mu for my mu my muscular for my muscular...', 'zh': '减轻酸... 酸痛的药 减轻酸痛的药'}, {'en': \"It's a challenge. God is challenging you. He's calling you a chump.\", 'zh': '上帝在挑战你，他说你是笨蛋'}, {'en': 'Oh, baby.', 'zh': '.. 寶貝'}, {'en': '- Lucinda?', 'zh': '- 盧辛達？'}])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "979ca916",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for example in ds[\"train\"]:    # <--- lặp dataset 'train' chứ không phải DatasetDict\n",
    "        en = example[\"translation\"][\"en\"]\n",
    "        zh = example[\"translation\"][\"zh\"]\n",
    "        f.write(en + \"\\n\")\n",
    "        f.write(zh + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96badc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = None\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30000,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "tokenizer.train(files=[\"corpus.txt\"], trainer=trainer)\n",
    "tokenizer.save(\"wordpiece.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61dfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.t5gemma.modeling_t5gemma import T5GemmaModuleConfig, T5GemmaModel\n",
    "from transformers.models.t5gemma.configuration_t5gemma import T5GemmaConfig\n",
    "# import tokenizers.\n",
    "# encoder = \n",
    "encoder_config = T5GemmaModuleConfig(\n",
    "    num_hidden_layers=4,\n",
    "    attention_dropout=0.2,\n",
    "    hidden_size=1024,\n",
    "    vocab_size=30000\n",
    ")\n",
    "decoder_config = T5GemmaModuleConfig(\n",
    "    num_hidden_layers=4,\n",
    "    attention_dropout=0.2,\n",
    "    hidden_size=1024,\n",
    "    vocab_size=30000\n",
    ")\n",
    "\n",
    "config = T5GemmaConfig(encoder=encoder_config, decoder=decoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10302d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5GemmaModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b49949c5-6a39-4831-9943-e086b56d96e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5GemmaModel(\n",
      "  (encoder): T5GemmaEncoder(\n",
      "    (embed_tokens): Embedding(30000, 1024, padding_idx=0)\n",
      "    (norm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): T5GemmaRotaryEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x T5GemmaEncoderLayer(\n",
      "        (self_attn): T5GemmaSelfAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        )\n",
      "        (pre_self_attn_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_self_attn_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (mlp): T5GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=1024, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (pre_feedforward_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (decoder): T5GemmaDecoder(\n",
      "    (embed_tokens): Embedding(30000, 1024, padding_idx=0)\n",
      "    (norm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "    (rotary_emb): T5GemmaRotaryEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x T5GemmaDecoderLayer(\n",
      "        (self_attn): T5GemmaSelfAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        )\n",
      "        (pre_self_attn_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_self_attn_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (mlp): T5GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=1024, out_features=9216, bias=False)\n",
      "          (up_proj): Linear(in_features=1024, out_features=9216, bias=False)\n",
      "          (down_proj): Linear(in_features=9216, out_features=1024, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (pre_feedforward_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (cross_attn): T5GemmaCrossAttention(\n",
      "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        )\n",
      "        (pre_cross_attn_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "        (post_cross_attn_layernorm): T5GemmaRMSNorm((1024,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df4e0940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77de2a74a7e04fb59189d030e96c7bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# load tokenizer đã train\n",
    "tokenizer = Tokenizer.from_file(\"wordpiece.json\")\n",
    "\n",
    "# cấu hình truncation/padding cho chiều dài cố định\n",
    "tokenizer.enable_truncation(max_length=128)\n",
    "tokenizer.enable_padding(length=128)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    # batch['translation'] là list of dicts {'en':..., 'zh':...}\n",
    "    sources = [t[\"en\"] for t in batch[\"translation\"]]\n",
    "    targets = [t[\"zh\"] for t in batch[\"translation\"]]\n",
    "\n",
    "    enc_src = tokenizer.encode_batch(sources)\n",
    "    enc_tgt = tokenizer.encode_batch(targets)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": [enc.ids for enc in enc_src],\n",
    "        \"attention_mask\": [enc.attention_mask for enc in enc_src],\n",
    "        \"labels\": [enc.ids for enc in enc_tgt],\n",
    "    }\n",
    "\n",
    "# Áp tokenizer lên split train (hoặc tất cả các split)\n",
    "tokenized_train = ds[\"train\"].map(tokenize_fn, batched=True, batch_size=1000)\n",
    "\n",
    "# Chuyển định dạng sang PyTorch tensors\n",
    "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0bde273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([29756, 20112, 19782, 19050, 18517,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([ 5474, 29629, 24596,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cb7dda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51bd12acd1e40029b6cb27fed69c91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 900000 Val samples: 100000\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "# Split tokenized dataset -> train/val, convert labels pad -> -100, set format và tạo DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 0.1\n",
    "SEED = 42\n",
    "\n",
    "# xác định id token [PAD]\n",
    "pad_id = tokenizer.token_to_id(\"[PAD]\") if tokenizer.token_to_id(\"[PAD]\") is not None else 0\n",
    "\n",
    "def _labels_to_ignore_index(batch):\n",
    "    # chuyển pad token trong labels thành -100 để loss ignore\n",
    "    batch[\"labels\"] = [[(t if t != pad_id else -100) for t in labs] for labs in batch[\"labels\"]]\n",
    "    return batch\n",
    "\n",
    "# áp chuyển đổi cho toàn bộ tokenized dataset (nếu chưa làm)\n",
    "tokenized_train = tokenized_train.map(_labels_to_ignore_index, batched=True, batch_size=1000)\n",
    "\n",
    "# chia train/val\n",
    "splits = tokenized_train.train_test_split(test_size=TEST_SIZE, seed=SEED)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "\n",
    "# set format sang torch tensors\n",
    "cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds.set_format(type=\"torch\", columns=cols)\n",
    "val_ds.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "# DataLoader (mặc định collate tốt vì đã padding)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Train samples:\", len(train_ds), \"Val samples:\", len(val_ds))\n",
    "# ...existing code..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
