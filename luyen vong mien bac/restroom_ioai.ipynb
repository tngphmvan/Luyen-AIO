{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0109bbbd",
   "metadata": {},
   "source": [
    "## Ghép Biểu Tượng Nhà Vệ Sinh\n",
    "\n",
    "### 1. Mô tả bài toán\n",
    "\n",
    "Nhiệm vụ này yêu cầu huấn luyện một mô hình để học mối quan hệ giữa **biểu tượng nam và nữ của cùng một nhà vệ sinh**. Dựa trên dữ liệu huấn luyện đã được gán nhãn, bạn cần huấn luyện một mô hình ghép sao cho, với một ảnh truy vấn, mô hình sẽ tìm được **biểu tượng khác giới** (ví dụ: ghép biểu tượng nam trong ảnh đã cắt với biểu tượng nữ trong ảnh gốc), với điều kiện cả hai biểu tượng đều thuộc **cùng một nhà vệ sinh**.\n",
    "\n",
    "Ví dụ, với một ảnh truy vấn (biểu tượng nam đã cắt) như Hình 1, đối tượng cần ghép là biểu tượng nữ trong ảnh gốc thuộc cùng nhà vệ sinh như Hình 2.\n",
    "\n",
    "| <img src=\"../IOAI-2025-main\\IOAI-2025-main\\Individual-Contest\\Restroom\\figs\\Restroom Fig 1.png\" height=\"200\"> | <img src=\"../IOAI-2025-main\\IOAI-2025-main\\Individual-Contest\\Restroom\\figs\\Restroom Fig 2.png\" height=\"200\"> |\n",
    "| -------------------------------------------------- | -------------------------------------------------- |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Bộ dữ liệu\n",
    "\n",
    "**(1) Tập huấn luyện:**\n",
    "\n",
    "Dùng để huấn luyện mô hình. Cấu trúc thư mục:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7346b",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "train/\n",
    "├── crop/         # Biểu tượng đã cắt\n",
    "│   ├── female/   1.png, 2.png, ...\n",
    "│   └── male/\n",
    "└── orig/         # Biểu tượng gốc\n",
    "    ├── female/\n",
    "    └── male/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad89949",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Mỗi thư mục con chứa các biểu tượng được đặt tên từ `1.png` đến `82.png`, số thứ tự là ID của nhà vệ sinh. Với mỗi nhà vệ sinh, có bốn ảnh (bốn ảnh này có cùng ID ở tất cả các thư mục con):\n",
    "\n",
    "- `crop/female/i.png` → Biểu tượng nữ đã cắt  \n",
    "- `crop/male/i.png`  → Biểu tượng nam đã cắt  \n",
    "- `orig/female/i.png` → Biểu tượng nữ gốc  \n",
    "- `orig/male/i.png`  → Biểu tượng nam gốc  \n",
    "\n",
    "**(2) Tập xác thực và tập kiểm tra:**\n",
    "\n",
    "Tập xác thực (`test_a`) hoặc tập kiểm tra (`test_b`) gồm hai thư mục con:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0673741",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "test_a/           (hoặc test_b/)\n",
    "├── query/        # Biểu tượng đã cắt cần ghép\n",
    "└── gallery/      # Tập hợp các biểu tượng gốc để ghép"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f250ac8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- `query/`: các biểu tượng đã cắt cần ghép  \n",
    "- `gallery/`: tập hợp các biểu tượng gốc để ghép  \n",
    "- Lưu ý:\n",
    "    - Không giống tập huấn luyện, tên file trong `query/` và `gallery/` được đánh số và xáo trộn độc lập, không thể ghép dựa vào ID\n",
    "    - Với mỗi biểu tượng đã cắt trong `query/` sẽ có đúng hai biểu tượng gốc trong `gallery/` (một nam, một nữ cùng nhà vệ sinh), tức là $\\text{len}(\\text{gallery})=2 * \\text{len}(\\text{query})$\n",
    "- Tất cả ảnh đều ở định dạng `.png`\n",
    "- Tập xác thực có $10$ ảnh trong thư mục `query/`, tập kiểm tra có $30$ ảnh trong thư mục `query/`\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Nhiệm vụ\n",
    "\n",
    "Với mỗi ảnh trong thư mục `query/`, dự đoán ảnh trong `gallery/` mà:\n",
    "- Là **khác giới** với ảnh truy vấn\n",
    "- Thuộc **cùng nhà vệ sinh**\n",
    "\n",
    "Việc ghép này phải được thực hiện **bằng mô hình đã huấn luyện**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Yêu cầu nộp bài\n",
    "\n",
    "Bạn phải nộp một notebook tên là `submission.ipynb`, gồm:\n",
    "\n",
    "- Quá trình huấn luyện mô hình (dùng dữ liệu `train/`)\n",
    "- Quá trình ghép cho cả hai tập kiểm tra (`test_a/` và `test_b/`)\n",
    "- Notebook phải xuất ra hai file `.npy`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888c680",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "submission_a.npy     # Kết quả ghép cho tập xác thực (test_a)\n",
    "submission_b.npy     # Kết quả ghép cho tập kiểm tra (test_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a13e10d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Mỗi file npy là một mảng một chiều, kích thước bằng số lượng truy vấn, mỗi giá trị là ID ảnh trong gallery.\n",
    "\n",
    "Ví dụ một file `submission_a.npy` hợp lệ:\n",
    "\n",
    "<img src=\"../IOAI-2025-main\\IOAI-2025-main\\Individual-Contest\\Restroom\\figs\\Restroom Fig 3.png\" width=\"300\">\n",
    "\n",
    "Bạn cũng có thể xem ví dụ output tại [baseline.ipynb](https://ioai.bohrium.com/notebooks/81153159178).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Chấm điểm\n",
    "\n",
    "- Nếu bài nộp hoàn thành trong thời gian quy định, điểm được tính như sau:  \n",
    "  $$\n",
    "  \\text{Score} = \\frac{\\text{Number of correct matches}}{\\text{Total queries}}\n",
    "  $$\n",
    "  Điểm là số thực từ $0.0$ đến $1.0$ (bao gồm cả hai đầu).\n",
    "- Nếu vượt quá thời gian, điểm sẽ là **0**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Baseline và tập huấn luyện\n",
    "\n",
    "- Có thể tham khảo giải pháp baseline bên dưới.\n",
    "- Bộ dữ liệu nằm trong thư mục `training_set`.\n",
    "- Điểm cao nhất của Ban Khoa học cho bài này là **0.90** trên Leaderboard B (dùng để chuẩn hóa điểm).\n",
    "- Điểm baseline của Ban Khoa học cho bài này là **0.77** trên Leaderboard B (dùng để chuẩn hóa điểm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0e34be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vitus\\miniconda3\\envs\\mavclip\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.clip import CLIPConfig, CLIPModel, CLIPTextConfig, CLIPVisionConfig\n",
    "# text_config = CLIPTextConfig()\n",
    "# vision_config = CLIPVisionConfig()\n",
    "config = CLIPConfig()\n",
    "clip_model = CLIPModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909fa5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import clip_loss, _get_vector_norm\n",
    "import torch\n",
    "def contrastive_loss(image_embeds, text_embeds):\n",
    "    image_embeds = image_embeds / _get_vector_norm(image_embeds)\n",
    "    text_embeds = text_embeds / _get_vector_norm(text_embeds)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = torch.nn.Parameter(torch.tensor(2.6592))\n",
    "    logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device))\n",
    "    logits_per_text = logits_per_text * logit_scale.exp().to(text_embeds.device)\n",
    "\n",
    "    logits_per_image = logits_per_text.t()\n",
    "    loss = clip_loss(logits_per_text)\n",
    "    return {\n",
    "        'loss': loss,\n",
    "        'logits': torch.argmax(logits_per_image, dim=1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db753ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.7974, grad_fn=<DivBackward0>), 'logits': tensor([1, 0])}\n"
     ]
    }
   ],
   "source": [
    "image_embeds = torch.rand((2, 520))  # batch size >= 2\n",
    "text_embeds = torch.rand((2, 520))\n",
    "print(contrastive_loss(image_embeds, text_embeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20cac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "class MyEffecientNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = efficientnet_b0(weights=None)\n",
    "    def forward(self, inputs):\n",
    "        return self.features(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c7ac68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "{'data': tensor([[[ 0.7591,  0.7419,  0.7248,  ...,  0.2796,  0.2967,  0.2967],\n",
      "         [ 0.7591,  0.7419,  0.7248,  ...,  0.2967,  0.3138,  0.3138],\n",
      "         [ 0.7419,  0.7419,  0.7248,  ...,  0.2796,  0.2967,  0.2967],\n",
      "         ...,\n",
      "         [ 0.1254,  0.1254,  0.1083,  ..., -0.0458, -0.0458, -0.0458],\n",
      "         [ 0.1083,  0.1083,  0.0912,  ..., -0.0458, -0.0458, -0.0458],\n",
      "         [ 0.0912,  0.0912,  0.0912,  ..., -0.0458, -0.0458, -0.0458]],\n",
      "\n",
      "        [[ 0.9580,  0.9405,  0.9230,  ...,  0.4853,  0.4678,  0.4678],\n",
      "         [ 0.9580,  0.9405,  0.9230,  ...,  0.5028,  0.4853,  0.4853],\n",
      "         [ 0.9580,  0.9405,  0.9230,  ...,  0.5028,  0.5028,  0.5028],\n",
      "         ...,\n",
      "         [ 0.5028,  0.4853,  0.4678,  ...,  0.2752,  0.2927,  0.2927],\n",
      "         [ 0.4853,  0.4678,  0.4503,  ...,  0.2752,  0.2752,  0.2752],\n",
      "         [ 0.4678,  0.4678,  0.4503,  ...,  0.2752,  0.2752,  0.2752]],\n",
      "\n",
      "        [[ 0.4265,  0.4091,  0.3916,  ..., -0.1835, -0.1835, -0.1661],\n",
      "         [ 0.4265,  0.4091,  0.3916,  ..., -0.1835, -0.1661, -0.1487],\n",
      "         [ 0.4265,  0.4091,  0.3916,  ..., -0.2010, -0.1835, -0.1661],\n",
      "         ...,\n",
      "         [-0.3055, -0.3055, -0.3230,  ..., -0.5670, -0.5670, -0.5670],\n",
      "         [-0.3230, -0.3230, -0.3055,  ..., -0.5495, -0.5495, -0.5495],\n",
      "         [-0.3404, -0.3230, -0.3055,  ..., -0.5321, -0.5321, -0.5495]]]), 'label': 42}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder, VisionDataset\n",
    "import os\n",
    "from pytorch_lightning import seed_everything\n",
    "from PIL import Image\n",
    "from torchvision.tv_tensors import Image as IMG\n",
    "# to_image = IMG()\n",
    "seed_everything(42, True, verbose=False)\n",
    "\n",
    "transform = EfficientNet_B0_Weights.DEFAULT.transforms()\n",
    "class MyDatasets(VisionDataset):\n",
    "    def __init__(self, root=None, transforms=None, transform=None, target_transform=None, query: str = 'male', gallery: str = 'female'):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.query = query\n",
    "        self.gallery = gallery\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_1 = Image.open(os.path.join(\n",
    "            self.root, f\"{self.query}/{index + 1}.png\")).resize((224, 224))\n",
    "        image_2 = Image.open(os.path.join(\n",
    "            self.root, f\"{self.gallery}/{index + 1}.png\")).resize((224, 224))\n",
    "        return {\n",
    "            'query': {\n",
    "                'data': self.transform(image_1),\n",
    "                'label': index\n",
    "            },\n",
    "            \n",
    "            'gallery': {\n",
    "                'data': self.transform(image_2),\n",
    "                'label': index\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(os.path.join(self.root, self.gallery)))\n",
    "\n",
    "dataset = MyDatasets(\n",
    "    root=r\"E:\\Luyen-AIO\\IOAI-2025-main\\IOAI-2025-main\\Individual-Contest\\Restroom\\training_set\\crop\", transform=transform)\n",
    "print(len(dataset))\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print(val_dataset[0]['query'])\n",
    "# plt.imshow(val_dataset[0]['query'].permute(1, 2, 0))\n",
    "# plt.show()\n",
    "# plt.imshow(val_dataset[0]['gallery'].permute(1, 2, 0))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f9bc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "class WrapperModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = MyEffecientNet()\n",
    "        self.loss = contrastive_loss\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        return self.model(batch['query']['data']), self.model(batch['gallery']['data'])\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.loss(self(batch)[0], self(batch)[1])\n",
    "        self.log(\"train_loss\", loss['loss'], prog_bar=True)\n",
    "        y_true = batch['gallery']['label']\n",
    "        y_pred = loss['logits'].long().detach().cpu().numpy()\n",
    "        acc = accuracy_score(y_true.cpu().numpy().flatten(), y_pred.flatten())\n",
    "        self.log(\"train_acc\", acc, on_epoch=True, on_step=False, prog_bar=True)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.loss(self(batch)[0], self(batch)[1])\n",
    "        self.log(\"val_loss\", loss['loss'], prog_bar=True)\n",
    "        y_true = batch['gallery']['label']\n",
    "        y_pred = loss['logits'].long().detach().cpu().numpy()\n",
    "        acc = accuracy_score(y_true.cpu().numpy().flatten(), y_pred.flatten())\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, on_step=False, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer,mode='min', patience=5)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'monitor': 'val_loss'\n",
    "            }\n",
    "        }\n",
    "\n",
    "trainer = Trainer(\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(dirpath='checkpoints/restroom_effecientnetb0', filename='best', save_last=True, monitor='val_loss', mode='min'),\n",
    "        EarlyStopping(mode='min', monitor='val_loss', patience=10)\n",
    "    ],\n",
    "    logger=[TensorBoardLogger(save_dir='tb_logs', name='restroom_effecientnetb0')],\n",
    "    max_epochs=1000, gradient_clip_val=1.0, max_time=\"00:00:15:00\", precision=32\n",
    ")\n",
    "model = WrapperModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2168d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, 36, True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, 36, True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aab329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = None\n",
    "# for ele in train_dataloader:\n",
    "#     print(ele['query'].dtype)\n",
    "#     print(ele['query'].shape)\n",
    "#     res = model(ele)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4f44af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0944f3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model | MyEffecientNet | 5.3 M  | train\n",
      "-------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.154    Total estimated model params size (MB)\n",
      "338       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2/2 [00:07<00:00,  0.26it/s, v_num=23, train_loss=3.550, val_loss=3.210, val_acc=0.000, train_acc=0.0154] \n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mavclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
