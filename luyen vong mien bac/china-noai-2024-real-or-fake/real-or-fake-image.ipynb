{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74389df-5bab-4963-852d-196b72b42edc",
   "metadata": {},
   "source": [
    "# Real or Fake Image Recognition Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0120f-2aee-45a1-87b2-5d8001f66343",
   "metadata": {},
   "source": [
    "> **Introduction:** This is the Question 2 of APOAI 2025 Mock Competition, and it is also the second question of the NOAI 2024 (China).\n",
    "\n",
    "## I. Question Overview\n",
    "\n",
    "CIFAR10 is a commonly used image classification data set. Each image is a 3x32x32 color image. Here, 3 represents the number of color channels, and 32x32 represents the image size. We have selected 5,000 training images and 1,000 test images from CIFAR10. These 6,000 images are obtained by taking pictures and are defined as \"real\" images.\n",
    "\n",
    "The Diffusion Model is a very powerful image generation model that can be used to generate \"fake\" images. We used a Diffusion Model on CIFAR10 to generate 6,000 \"fake\" images, among which 5,000 \"fake\" images are for the training set and 1,000 are for the test set. The samples in the test set cannot be accessed during the competition.\n",
    "\n",
    "## II. Data Set\n",
    "\n",
    "1. Address of the training set: [Training Set](https://bohrium.dp.tech/competitions/2623226705?tab=datasets).\n",
    "    - The real pictures of the training set are stored in the `train/cifar` folder.\n",
    "    - The fake pictures of the training set are stored in the `train/uvit` folder.\n",
    "2. Test set (cannot be directly downloaded during the competition):\n",
    "    - The real pictures of the test set are stored in the `test/cifar` folder.\n",
    "    - The fake pictures of the test set are stored in the `test/uvit` folder.\n",
    "\n",
    "## III. Task\n",
    "\n",
    "Please use PyTorch to design a **Convolutional Neural Network** to implement model training and testing, which is used to distinguish which image is a \"real\" image and which is a \"fake\" image.\n",
    "\n",
    "The specific requirements are as follows:\n",
    "\n",
    "1. During the training process, please set the Label of the \"real\" image to 0 by yourself, and set the Label of the \"fake\" image to 1.\n",
    "2. Name the class of the model as `MyModel()`. Using other names may lead to failure in submission.\n",
    "3. Include at least 2 convolutional layers (`nn.Conv2d`) and 2 max pooling layers (`nn.MaxPool2d`). Do not use other methods to define convolutional layers and pooling layers. Please build the neural network directly and do not use `nn.Sequential()` nesting. The scoring system cannot detect the network structure inside `nn.Sequential()`, and a score of 0 will be directly given.\n",
    "4. Include at most 2 linear layers (`nn.Linear`). Please build the neural network directly and do not use `nn.Sequential()` nesting. The scoring system cannot detect the network structure inside `nn.Sequential()`, and a score of 0 will be directly given.\n",
    "5. The activation function of each layer can only be selected from `nn.ReLU`, `nn.Sigmoid`, `nn.Tanh`, `nn.ELU`, `nn.LeakyReLU`, `nn.PreLU`.\n",
    "6. The loss function, optimizer (solver), and learning rate can be freely selected.\n",
    "\n",
    "## IV. Submission\n",
    "\n",
    "Please submit a compressed file named `submission.zip`. After decompression, it should contain the model file `submission_model.py` and the model parameter file `submission_dic.pth`. The specific requirements are as follows:\n",
    "\n",
    "1. Save the class definition of the model and the required precursor libraries in `submission_model.py`.\n",
    "2. Save the trained model parameters in `submission_dic.pth`. The model parameters will be loaded during scoring.\n",
    "3. You can refer to the method in `baseline.ipynb` to generate the `submission.zip` file on the platform for submission. You can also download the data set to the local machine, train the model, and then package it into a `submission.zip` file for submission.\n",
    "\n",
    "> **Address of `baseline.ipynb`:** [Question 2 of APOAI2025 Mock Competition_baseline](https://bohrium.dp.tech/notebooks/53453886135/).\n",
    "\n",
    "## V. Scoring\n",
    "\n",
    "1. When the number of linear layers and the number of neurons in the neural network meet the requirements, the score is calculated as follows:\n",
    "    1. Network complexity score: `Network_Simplicity_Score = 1 / (Num_Linear + Num_Conv + 1)`. \\\n",
    "       Here, `Num_Linear` is the number of linear layers; `Num_Conv` is the number of convolutional layers.\n",
    "    2. The accuracy rate of the model on the test set: Accuracy (Now the metrics has bug to double the accuracy =v=).\n",
    "    3. Final score: `Score = (Network_Simplicity_Score + Accuracy) * 3/4`.\n",
    "2. When the number of linear layers and the number of neurons in the neural network do not meet the requirements, the score is 0.\n",
    "\n",
    "> **Remarks:** The leaderboard A uses 50% of the data in the test set, which can be displayed in real time during the competition to help contestants debug the model. The leaderboard B uses the remaining 50% of the data in the test set and is calculated after the competition ends. The score of the leaderboard B is the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d24eb2-1c0a-4dbf-bb81-2244bd02ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "seed = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aead48-4a2e-4623-b6af-5b7de893b32a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cd3c1a4-0dc0-4f9f-adf8-4fc390f25802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        for label, sub_dir in enumerate(os.listdir(root_dir)):  # cifar has label 0, uvit has label 1\n",
    "            full_dir = os.path.join(root_dir, sub_dir)\n",
    "            for img_name in os.listdir(full_dir):\n",
    "                img_path = os.path.join(full_dir, img_name)\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "820c7e63-1ecf-4a12-8840-0cc91c210365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # These values are used when grading\n",
    "])\n",
    "ds_train = CustomDataset(r\"../../noai/malaysia/ioai-tsp-2025-main/noai-china-2024/real-or-fake-image/train_v1\", transform=transform)\n",
    "dl_train = DataLoader(ds_train, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62f023-e0a2-42bc-b480-2057f82b3a54",
   "metadata": {},
   "source": [
    "## Define model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a374882f-3035-4a26-a342-6982925f726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 10, 5)\n",
    "        self.fc1 = nn.Linear(10 * 5 * 5, 70)\n",
    "        self.fc2 = nn.Linear(70, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c50bf9b0-57c5-4334-a1e5-1a2158b52944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "def train(model, device, optimizer, criterion, dataloader, num_epochs):\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        \n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "            \n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        running_loss /= len(dataloader.dataset)\n",
    "        train_losses.append(running_loss)\n",
    "        accuracy = eval(model, device, dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss}, Accuracy: {accuracy}\")\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d0580f-a647-4b83-b002-001c886ece29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval(model, device, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device).float()\n",
    "        \n",
    "        outputs = model(images).squeeze()\n",
    "        preds = outputs >= 0.5\n",
    "        \n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b89c87d-48d9-4516-8927-409006b77c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model = MyModel().to(device)\n",
    "model = resnet18(weights=None)\n",
    "model.fc = nn.Linear(512, 1, True)\n",
    "model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6973e35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "816a9ee2-9d42-4c3b-ae19-550cd75b3d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:22<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5968762973308563, Accuracy: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:21<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.5578853813171387, Accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:22<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.5376831862449646, Accuracy: 0.7341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:23<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.529094282913208, Accuracy: 0.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:23<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.5089563994884491, Accuracy: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:23<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.48770226249694826, Accuracy: 0.7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:23<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.4604527988433838, Accuracy: 0.8018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:23<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.4443097314834595, Accuracy: 0.7973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:23<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.41340775609016417, Accuracy: 0.8351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:22<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.38329280331134796, Accuracy: 0.8221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5968762973308563,\n",
       " 0.5578853813171387,\n",
       " 0.5376831862449646,\n",
       " 0.529094282913208,\n",
       " 0.5089563994884491,\n",
       " 0.48770226249694826,\n",
       " 0.4604527988433838,\n",
       " 0.4443097314834595,\n",
       " 0.41340775609016417,\n",
       " 0.38329280331134796]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pytorch_lightning import Trainer\n",
    "# trainer = Trainer()\n",
    "# trainer.fit(model, dl_train)\n",
    "train(model, device, optimizer, criterion, dl_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704fb02-25f9-43ad-8761-409daafa7614",
   "metadata": {},
   "source": [
    "## Save for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2af090a9-38ee-4fd9-918b-02b4e9032940",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_filename = \"submission_model.py\"\n",
    "pth_filename = \"submission_dic.pth\"\n",
    "zip_filename = \"submission.zip\"  # Will submit this zip to the grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7238f8ef-411d-4287-b111-ec504742321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), pth_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd4888b8-1432-4a51-b27a-66d0205e3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = \"\"\"  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 10, 5)\n",
    "        self.fc1 = nn.Linear(10 * 5 * 5, 70)\n",
    "        self.fc2 = nn.Linear(70, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 10 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\"\"\".lstrip()\n",
    "\n",
    "with open(py_filename, \"w\") as f:\n",
    "    f.write(model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13d1ec-dbd0-4309-b77d-2af688da4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
    "    for file in [py_filename, pth_filename]:\n",
    "        zipf.write(file, os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ecb094-2229-43bd-ac17-b40b27a9e4f5",
   "metadata": {},
   "source": [
    "## Score\n",
    "\n",
    "Leaderboard A accuracy: 1.0000\n",
    "\n",
    "Leaderboard B accuracy: 1.0000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mavclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
