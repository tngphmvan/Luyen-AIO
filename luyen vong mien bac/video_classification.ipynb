{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "360fe86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torchvision.tv_tensors import Video, Image\n",
    "from torchvision.ops import Conv2dNormActivation, DeformConv2d\n",
    "import os\n",
    "import torch\n",
    "# from torchvision.datasets.video_utils import read_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1f12ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_CLASSES = 100\n",
    "TARGET_FRAMES = 16  # number of frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea554a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read video frames using OpenCV\n",
    "def read_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(f\"Could not read any frames from {video_path}\")\n",
    "    frames = torch.from_numpy(np.stack(frames, axis=0))\n",
    "    return frames\n",
    "\n",
    "\n",
    "# Custom collate function for batching\n",
    "def collate_fn(batch):\n",
    "    frames = torch.stack([item['frames'] for item in batch])\n",
    "    labels = torch.tensor([item['label_idx'] for item in batch])\n",
    "    label_names = [item['label'] for item in batch]\n",
    "    return {'frames': frames, 'label_idx': labels, 'label': label_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38c59f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "import unicodedata\n",
    "\n",
    "# Define video dataset\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_to_idx_path, transform=None,\n",
    "                 mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n",
    "                 target_frames=32):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mean, self.std = mean, std\n",
    "        self.target_frames = target_frames\n",
    "        self.instances, self.labels, self.label_idx = [], [], []\n",
    "\n",
    "        with open(label_to_idx_path, 'rb') as f:\n",
    "            raw_mapping = pickle.load(f)\n",
    "            # Chuẩn hóa key trong mapping về NFC\n",
    "            self.label_mapping = {unicodedata.normalize('NFC', k): v for k, v in raw_mapping.items()}\n",
    "\n",
    "        # Lấy danh sách folder và chuẩn hóa tên để so sánh\n",
    "        folder_names = sorted(os.listdir(root_dir))[:NUM_CLASSES]\n",
    "        \n",
    "        for label_folder in folder_names:\n",
    "            # Chuẩn hóa tên folder về NFC\n",
    "            norm_label = unicodedata.normalize('NFC', label_folder)\n",
    "            \n",
    "            path = os.path.join(root_dir, label_folder)\n",
    "            if os.path.isdir(path):\n",
    "                if norm_label not in self.label_mapping:\n",
    "                    print(f\"Bỏ qua: '{label_folder}' (Normalized: '{norm_label}') không có trong mapping\")\n",
    "                    continue\n",
    "\n",
    "                idx = self.label_mapping[norm_label]\n",
    "                \n",
    "                for video_file in os.listdir(path):\n",
    "                    video_path = os.path.join(path, video_file)\n",
    "                    self.instances.append(video_path)\n",
    "                    self.labels.append(norm_label)\n",
    "                    self.label_idx.append(idx)\n",
    "\n",
    "    # Downsample frames to fixed length\n",
    "    def _downsample_frames(self, frames):\n",
    "        num_frames = frames.shape[0]\n",
    "        if num_frames == self.target_frames:\n",
    "            return frames\n",
    "        elif num_frames < self.target_frames:\n",
    "            pad = self.target_frames - num_frames\n",
    "            return torch.cat([frames, frames[-1:].repeat(pad, 1, 1, 1)], dim=0)\n",
    "        else:\n",
    "            idx = torch.linspace(0, num_frames - 1, self.target_frames).long()\n",
    "            return frames[idx]\n",
    "\n",
    "    # Normalize frames with ImageNet stats\n",
    "    def _normalize(self, frames):\n",
    "        frames = frames.permute(0, 3, 1, 2).float() / 255.0\n",
    "        mean = torch.tensor(self.mean).view(1, 3, 1, 1)\n",
    "        std = torch.tensor(self.std).view(1, 3, 1, 1)\n",
    "        return (frames - mean) / std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.instances[idx]\n",
    "        label, label_idx = self.labels[idx], self.label_idx[idx]\n",
    "        frames = read_video(video_path)\n",
    "        frames = self._downsample_frames(frames)\n",
    "        frames = self._normalize(frames)\n",
    "        return {\"frames\": frames, \"label_idx\": label_idx, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "827f56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders: ['An ủi', 'Ban ngày', 'Ban đêm', 'Biết', 'Biếu tặng']\n",
      "Mapping keys: ['An ủi', 'Áp dụng', 'Ăn', 'Ăn mừng', 'Ban ngày']\n",
      "Missing in mapping: ['An ủi', 'Ban ngày', 'Ban đêm', 'Biết', 'Biếu tặng', 'Bàn tay', 'Băn khoăn', 'Bạn thân', 'Bế mạc', 'Bệnh nhân', 'Bệnh viện', 'Bộ y tế', 'Chiều', 'Chào', 'Chân', 'Chúng ta', 'Chạy', 'Chấp nhận', 'Chậm lại', 'Con gấu', 'Cá', 'Cách ly', 'Cám dỗ', 'Có thể', 'Cơ thể', 'Cảm ơn', 'Cần', 'Cứu', 'Dạy dỗ', 'Dễ', 'Ghét', 'Giúp', 'Hâm mộ', 'Hôm nay', 'Họ', 'Học sinh', 'Khai báo', 'Khu cách ly', 'Khóc', 'Khẩu trang', 'Kết hôn', 'Lo lắng', 'Lây bệnh', 'Mời vào', 'Nghỉ ngơi', 'Ngón tay', 'Nhà', 'Nhìn', 'Nhầm', 'Nhớ', 'Nói', 'Nói xấu', 'Nôn ói', 'Nặng', 'Phía sau', 'Phạt', 'Phỏng vấn', 'Phục hồi', 'Rẽ phải', 'Rẽ trái', 'San sẻ', 'Sốt', 'Sử dụng', 'Thích', 'Thăm', 'Thương', 'Thất lạc', 'Thức dậy', 'Thức ăn', 'Trưa', 'Trường học', 'Tôi', 'Tập luyện', 'Tối', 'Uống', 'Vâng lời', 'Xe máy', 'Xe đạp', 'Xin lỗi', 'Xin phép', 'Xuất viện', 'Xúc động', 'Áp dụng', 'Ô tô', 'Ăn', 'Ăn mừng', 'Đâu', 'Đầu', 'Đẹp', 'Đồng ý', 'Ủng hộ']\n"
     ]
    }
   ],
   "source": [
    "# Debug: kiểm tra mapping vs folders\n",
    "with open(\"/kaggle/input/cv-vng-min-bc/dataset/label_mapping.pkl\", 'rb') as f:\n",
    "    label_mapping = pickle.load(f)\n",
    "\n",
    "folders = sorted(os.listdir(\"/kaggle/input/cv-vng-min-bc/dataset/train\"))[:NUM_CLASSES]\n",
    "print(\"Folders:\", folders[:5])\n",
    "print(\"Mapping keys:\", list(label_mapping.keys())[:5])\n",
    "print(\"Missing in mapping:\", [f for f in folders if f not in label_mapping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138e40d",
   "metadata": {},
   "source": [
    "# Video Classification TorchVision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d25ddde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.video import mvit_v1_b, MViT_V1_B_Weights\n",
    "model = mvit_v1_b(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0163c929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.head = torch.nn.Linear(in_features=768, out_features=100, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d1cee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VideoDataset(root_dir=\"/kaggle/input/cv-vng-min-bc/dataset/train\", transform=MViT_V1_B_Weights.DEFAULT.transforms(), label_to_idx_path=\"/kaggle/input/cv-vng-min-bc/dataset/label_mapping.pkl\", target_frames=TARGET_FRAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3929cfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.utils.data import random_split\n",
    "import pytorch_lightning as pl\n",
    "pl.seed_everything(42, workers=True)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba4a549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814\n",
      "3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2659/1833639733.py:5: RuntimeWarning: divide by zero encountered in divide\n",
      "  class_weights = 1. / np.bincount(classes_indices)\n"
     ]
    }
   ],
   "source": [
    "classes_indices = [ele for ele in train_dataset.dataset.label_idx if ele in train_dataset.indices]\n",
    "print(len(classes_indices))\n",
    "print(len(dataset))\n",
    "import numpy as np\n",
    "class_weights = 1. / np.bincount(classes_indices)\n",
    "samples_weights = [class_weights[ele] for ele in classes_indices]\n",
    "sampler = WeightedRandomSampler(weights=samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False, num_workers=2, collate_fn=collate_fn, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccebad6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MViT(\n",
      "  (conv_proj): Conv3d(3, 96, kernel_size=(3, 7, 7), stride=(2, 4, 4), padding=(1, 3, 3))\n",
      "  (pos_encoding): PositionalEncoding()\n",
      "  (blocks): ModuleList(\n",
      "    (0): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=96, out_features=96, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 8, 8), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 8, 8), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=384, out_features=192, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "      (project): Linear(in_features=96, out_features=192, bias=True)\n",
      "    )\n",
      "    (1): MultiscaleBlock(\n",
      "      (pool_skip): Pool(\n",
      "        (pool): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (pool_q): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.013333333333333334, mode=row)\n",
      "    )\n",
      "    (2): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=768, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.02666666666666667, mode=row)\n",
      "      (project): Linear(in_features=192, out_features=384, bias=True)\n",
      "    )\n",
      "    (3): MultiscaleBlock(\n",
      "      (pool_skip): Pool(\n",
      "        (pool): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_q): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.04000000000000001, mode=row)\n",
      "    )\n",
      "    (4): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.05333333333333334, mode=row)\n",
      "    )\n",
      "    (5): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.06666666666666667, mode=row)\n",
      "    )\n",
      "    (6): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.08000000000000002, mode=row)\n",
      "    )\n",
      "    (7): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.09333333333333334, mode=row)\n",
      "    )\n",
      "    (8): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.10666666666666667, mode=row)\n",
      "    )\n",
      "    (9): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.12000000000000001, mode=row)\n",
      "    )\n",
      "    (10): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.13333333333333333, mode=row)\n",
      "    )\n",
      "    (11): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.14666666666666667, mode=row)\n",
      "    )\n",
      "    (12): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.16000000000000003, mode=row)\n",
      "    )\n",
      "    (13): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=1536, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.17333333333333334, mode=row)\n",
      "      (project): Linear(in_features=384, out_features=768, bias=True)\n",
      "    )\n",
      "    (14): MultiscaleBlock(\n",
      "      (pool_skip): Pool(\n",
      "        (pool): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (pool_q): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.18666666666666668, mode=row)\n",
      "    )\n",
      "    (15): MultiscaleBlock(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): MultiscaleAttention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (project): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (pool_k): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (pool_v): Pool(\n",
      "          (pool): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)\n",
      "          (norm_act): Sequential(\n",
      "            (0): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Dropout(p=0.0, inplace=False)\n",
      "        (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (4): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): Linear(in_features=768, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd468b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pytorch-lightning\n",
      "Version: 2.5.5\n",
      "Summary: PyTorch Lightning is the lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.\n",
      "Home-page: https://github.com/Lightning-AI/lightning\n",
      "Author: Lightning AI et al.\n",
      "Author-email: developer@lightning.ai\n",
      "License: Apache-2.0\n",
      "Location: /usr/local/lib/python3.11/dist-packages\n",
      "Requires: fsspec, lightning-utilities, packaging, PyYAML, torch, torchmetrics, tqdm, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0d589f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['frames'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15f85aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics import F1Score\n",
    "F1Score = F1Score(num_classes=NUM_CLASSES, average='macro', task='multiclass')\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class WrapperModel(LightningModule):\n",
    "    def __init__(self, lr=1e-2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = model\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.f1 = F1Score\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        frames, labels = torch.permute(batch['frames'], (0, 2, 1, 3, 4)), batch['label_idx']\n",
    "        outputs = self(frames)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        f1 = self.f1(outputs, labels)\n",
    "        self.log('train_f1', f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        frames, labels = torch.permute(batch['frames'], (0, 2, 1, 3, 4)), batch['label_idx']\n",
    "        outputs = self(frames)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        f1 = self.f1(outputs, labels)\n",
    "        self.log('val_f1', f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"fitness\", loss*0.6 + f1*0.4, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), self.lr)\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler, \"monitor\": \"fitness\"}\n",
    "\n",
    "model_wrapper = WrapperModel(lr=1e-3)\n",
    "trainer = Trainer(max_epochs=20, accelerator='gpu', devices=2,\n",
    "                  callbacks=[ModelCheckpoint(monitor='fitness', mode='min', save_top_k=1),\n",
    "                             EarlyStopping(monitor='fitness', mode='min', patience=5)], max_time=\"00:01:00:00\",\n",
    "                  logger=TensorBoardLogger(\"tb_logs\", name=\"video_classification\"),gradient_clip_val=1.0, accumulate_grad_batches=4)\n",
    "    \n",
    "# trainer.fit(model_wrapper, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43221b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet152_Weights, resnet152\n",
    "import torch\n",
    "resnet = resnet152(weights=ResNet152_Weights.IMAGENET1K_V1)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "hola = torch.nn.Sequential(*list(resnet.children())[:-1])(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fed6432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hola.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "188377b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CRNN model\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes=100, hidden_size=256):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # self.feature_dim = 2048\n",
    "        # self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        transformers_layer = nn.TransformerEncoderLayer(d_model=2048, nhead=8, batch_first=True, activation='gelu', dropout=0.2)\n",
    "        self.transformer = nn.TransformerEncoder(transformers_layer, num_layers=2, norm=nn.LayerNorm(2048))\n",
    "        # self.rnn = nn.LSTM(self.feature_dim, hidden_size, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.reshape(B * T, C, H, W)\n",
    "        # print(x.shape)\n",
    "        with torch.no_grad():\n",
    "            features = self.cnn(x)\n",
    "        # print(features.shape)\n",
    "        # pooled = self.pool(features).squeeze(-1).squeeze(-1)\n",
    "        seq = features.reshape(B, T, 2048)\n",
    "        # print(seq.shape)\n",
    "        res = self.transformer(seq)\n",
    "        # print(\"transformers res\", res.shape)\n",
    "        final = res.mean(dim=1)\n",
    "        # print(\"final\", final.shape)\n",
    "        return self.fc(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1288dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n"
     ]
    }
   ],
   "source": [
    "# Debug model\n",
    "# del model\n",
    "model = CRNN()\n",
    "dummy_input = torch.randn(2, 16, 3, 224, 224)\n",
    "output = model(dummy_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ccd4e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "W1204 10:53:51.152000 2659 torch/multiprocessing/spawn.py:169] Terminating process 2818 via signal SIGTERM\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 598, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 967, in _run\n    self.strategy.setup_environment()\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/ddp.py\", line 154, in setup_environment\n    self.setup_distributed()\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/ddp.py\", line 206, in setup_distributed\n    _init_dist_connection(self.cluster_environment, self._process_group_backend, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/lightning_fabric/utilities/distributed.py\", line 298, in _init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n    func_return = func(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1714, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/rendezvous.py\", line 274, in _env_rendezvous_handler\n    store = _create_c10d_store(\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/rendezvous.py\", line 194, in _create_c10d_store\n    return TCPStore(\n           ^^^^^^^^^\nRuntimeError: The server socket has failed to listen on any local network address. port: 40489, useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2659/2579885514.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                   logger=TensorBoardLogger(\"tb_logs\", name=\"crnn_transformers\"),gradient_clip_val=1.0, accumulate_grad_batches=4, profiler=\"simple\", )\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprocess_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout, grace_period)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 173, in _wrapping_function\n    results = function(*args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 598, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 967, in _run\n    self.strategy.setup_environment()\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/ddp.py\", line 154, in setup_environment\n    self.setup_distributed()\n  File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/ddp.py\", line 206, in setup_distributed\n    _init_dist_connection(self.cluster_environment, self._process_group_backend, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/lightning_fabric/utilities/distributed.py\", line 298, in _init_dist_connection\n    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 81, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/c10d_logger.py\", line 95, in wrapper\n    func_return = func(*args, **kwargs)\n                  ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/distributed_c10d.py\", line 1714, in init_process_group\n    store, rank, world_size = next(rendezvous_iterator)\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/rendezvous.py\", line 274, in _env_rendezvous_handler\n    store = _create_c10d_store(\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/rendezvous.py\", line 194, in _create_c10d_store\n    return TCPStore(\n           ^^^^^^^^^\nRuntimeError: The server socket has failed to listen on any local network address. port: 40489, useIpv6: 0, code: -98, name: EADDRINUSE, message: address already in use\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics import F1Score\n",
    "F1Score = F1Score(num_classes=NUM_CLASSES, average='macro', task='multiclass')\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, StochasticWeightAveraging\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class WrapperModel(LightningModule):\n",
    "    def __init__(self, lr=1e-2, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # self.feature_dim = 2048\n",
    "        # self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        transformers_layer = nn.TransformerEncoderLayer(d_model=2048, nhead=8, batch_first=True, activation='gelu', dropout=0.2)\n",
    "        self.transformer = nn.TransformerEncoder(transformers_layer, num_layers=2, norm=nn.LayerNorm(2048))\n",
    "        # self.rnn = nn.LSTM(self.feature_dim, hidden_size, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(2048, 100)\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self.f1 = F1Score\n",
    "        self.lr = lr\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        # print(x.shape)\n",
    "        x = x.reshape(B * T, C, H, W)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.cnn(x)\n",
    "        # print(features.shape)\n",
    "        # pooled = self.pool(features).squeeze(-1).squeeze(-1)\n",
    "        seq = features.reshape(B, T, 2048)\n",
    "        # print(seq.shape)\n",
    "        res = self.transformer(seq)\n",
    "        # print(\"transformers res\", res.shape)\n",
    "        final = res.mean(dim=1)\n",
    "        # print(\"final\", final.shape)\n",
    "        return self.fc(final)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        frames, labels = batch['frames'], batch['label_idx']\n",
    "        outputs = self(frames)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        f1 = self.f1(outputs, labels)\n",
    "        self.log('train_f1', f1, prog_bar=True, on_step=True)\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        frames, labels = batch['frames'], batch['label_idx']\n",
    "        outputs = self(frames)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        f1 = self.f1(outputs, labels)\n",
    "        self.log('val_f1', f1, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=True, on_epoch=False)\n",
    "        self.log(\"fitness\", loss*0.6 + f1*0.4, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), self.lr)\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler, \"monitor\": \"fitness\"}\n",
    "\n",
    "model_wrapper = WrapperModel(lr=1e-3)\n",
    "trainer = Trainer(max_epochs=20, accelerator='gpu', devices=2,\n",
    "                  callbacks=[ModelCheckpoint(monitor='fitness', mode='min', save_top_k=1),\n",
    "                             EarlyStopping(monitor='fitness', mode='min', patience=5), StochasticWeightAveraging(1e-6)], max_time=\"00:01:00:00\",\n",
    "                  logger=TensorBoardLogger(\"tb_logs\", name=\"crnn_transformers\"),gradient_clip_val=1.0, accumulate_grad_batches=4, profiler=\"simple\", )\n",
    "    \n",
    "trainer.fit(model_wrapper, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e93e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(10, 32, 512)\n",
    "lstm = nn.LSTM(512, 512, batch_first=True, dropout=0.3)\n",
    "out = lstm(src)\n",
    "out[:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(torch.squeeze)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
